
.. VISION ..

Close to the metal, C-like machine control, but with the more useful constructs for higher-level and functional programming from Python (without going too crazy).
In a way, it would be an attempt to make pythonic constructs fast, i.e: make them usable for the Long Descent.
It seems logical to have two 'sub-languages':
- A JAI-style low level strongly typed basic syntax, with the most minimal data structures (this includes basically first-class structs, unions, enums, arrays & strings).
  The total code surface for this must be really really minimal.
- A pythonic higher level syntax that builds on top of the previous one, with more advanced (publicly reviewed) data structures in a 'standard library' that is actually good.
  Any abstractions must be really straightforward and easy to 'unravel', code should still be very small.
  Still this could be fairly opinionated about stuff like how memory is allocated, like for instance having pre-defined allocators to choose from and use handles instead of pointers for everything.
- A third tier would be a more generic library of useful 'tools' that would be completely optional and offered just to quickly kickstart new programs etc.
All layers should be pretty independent of one another, specially the last one, but ideally also the second layer would be removable, even though it would be included by default.
Think where things like polymorphism & generics fall in this division.


________________________________________________________________________________

- Functions are a first class type and can be declared anywhere except inside data definitions, including other functions.
  Recover that idea in one of the early JAI videos about being able to slowly promote functions from quick anonymous lambdas to full blown
  independent functions while using the same exact syntax all along. Go find that video and write down the details.

- Have some syntax to support function signatures that must change in sync in several places in the code: i.e. platform interfaces, plugins, all kinds of interfaces really.
  What we'd want to do here is still be able to actually _see_ what the signature is while dealing with the various declarations/definitions.
  The current #define trick in C++ makes that all very opaque.

- Carefully consider NOT supporting function overloading?
  Pros:
    · It's just very convenient!
  Cons:
    · It makes code harder to follow, since a simple search by function name brings up potentially totally unrelated calls

- Consider marking any function arguments that can modify the source value with the 'ref' keyword, and make all others immutable.

- Maybe even do something like in Python where there is a kind of "correspondence" between function args and struct fields,
  and you can trivially pack/unpack structs as function args

- Bake the concept of _handles_ in the type system, as a better pointer anywhere it makes sense (with the possibility of obtaining the actual pointer for speed when desired)
  In fact lately I've been thinking that on 64 bit machines specially, using 32 bit handles where possible would not only be safer, but also save a lot of memory!
  Think of ways in which the language could promote handles and "discourage" straight pointer use.

  There are interesting ways in which several bits of a 64-bit pointer could be reused:
  https://en.wikipedia.org/wiki/Tagged_pointer
  https://stackoverflow.com/questions/16198700/using-the-extra-16-bits-in-64-bit-pointers
  https://stackoverflow.com/questions/62008371/encode-additional-information-in-pointer
  https://nikic.github.io/2012/02/02/Pointer-magic-for-efficient-dynamic-value-representations.html
  (NOTE all this stuff is probably very non-portable)

- This stuff about "Low fat pointers" for bounds checking seems interesting too
  https://pure.royalholloway.ac.uk/portal/files/27687028/ndss2017.pdf
  https://github.com/GJDuck/LowFat

- For arrays and strings, there is a question of what do the types actually represent in memory. Is an array a fat pointer to the actual list of items, or is it a normal pointer to the
  items with a preamble in front with the necessary metadata (basically just a count)? 
  Fat pointer:
    · Faster access to the count, meaning range checks in debug will be faster
    · Much easier to interop with stuff returned from C. Just create a fat wrapper to the actual pointer returned.
    · Fat! Fat pointers are really fat on 64bit, so arrays of arrays and arrays of strings waste a ton of bits for no real reason.
  Normal pointer:
    · Hard to interop with. A plain old buffer in memory cannot be converted into this without an extra copy.
    · Slimer, and easier to turn into something even more slim like a small handle, as there's no metadata to drag around.

  Think about how different subtypes of arrays could specify sizes for the base pointer as part of the type definition, similar to the "relative pointers" concept from JAI.
  Maybe also the base pointer could instead be a smaller handle, but then we'd need a concept of a non-opaque handle where the language knows what's inside and can use it.

- After thinking about it, there seem to be _three_ types of "arrays" actually:
  · There's the traditional C-like static array, with a fixed count (i.e. [3]int). It's layout is just the inlined elements in memory, and its count is a constant expression and immutable
    so its known internally by the compiler and there's no need to store it anywhere in memory.
  · There's 'sbuffers' or dynamic variable arrays, which store a count, a capacity, and a reference to a changeable location in memory where the actual contents are. The layout of a variable
    of this type is just those 3 elements, so they're relatively cheap to copy and pass around.
  · Then there's arrays of 'unknown' kind, represented by empty brackets [], used to pass both arrays of any size and dynamic arrays to functions. Empty brackets used when declaring a variable
    are not quite the same thing, as those just mean the size is specified by the initialization expression instead, but they're just a static array in the end.

    This third kind is more akin to a generic 'buffer view' (or just 'buffer'), basically a pointer and a count. If we do it this way, it'd mean that when passing a static array the compile-time size
    would have to be passed during runtime into the argument. On the other hand, when passing an sbuffer its current count would be passed, meaning only the current contents up to the count
    would be visible by the function.
    It's an open question whether we want to permit this array kind to ever be stored in a struct or whatever. It seems in principle a 'buffer view' could be a useful thing to have, although
    if we do the syntax should be more clear, as right now it's exactly the same as when initializing a static array of unknown size, so maybe something like [?] or [*] makes more sense.
    I can see a few use cases in favor of storing this as a more general type:
      - It's the perfect substitute for the pointer + size pair used so much in C when reading input data of an unknown size or when allocating a buffer with a size calculated at runtime.
      - Sometimes we might want to use an sbuffer during a process but then 'fix' the final result and signal it's not going to change any more. This provides an idiom to do so, although
        it's clearly a minor thing.

- Have a think about the pseudo rant Casey makes in https://www.twitch.tv/videos/999392183?t=03h27m43s about how our concept of buffers is no good and how they should be "intrusive" instead.
    The gist of it, in case the video goes away, is that a buffer should instead be a "pointer with markup", where you are essentially augmenting the pointer with an indication of where to find
    the range's limit, and not have it be part of the type itself and copied all around. To be clear, the way I understand it, the indication itself is a constexpr, so I guess something like:

    A :: struct
    {
        array: [count]int;
        count: int;
    }

    where 'count' inside the brackets is not saying the buffer is declared as having as many elements as the value of count at that point. It's saying that's where to find the number of elements _at all times_.
    I don't immediately see how you'd extend this concept generally for a count that's not defined next to the array itself (which is just equivalent to just having both things be a new type), but it sounds intriguing,
    so have a think about it.


- Precedence table, like in Bitwise's Ion, highest to lowest:
  · postfix: () [] .
  · unary: ! + - ~ &(addr) *(deref)
  · mul: * / % << >> &
  · add: + - | ^
  · cmp: == != < > <= >=
  · and: &&
  · or: ||
  · ternary: ?
  · comma: ,

- Modules/packages are the main way of organizing code. All .do files in the same directory are automatically part of the same module. Code can be freely moved from one big file to several separate smaller ones (or the other way around) _with no other action necessary_.
  Modules are imported all at once by specifying the name of the folder.
  https://bitwise.handmade.network/episode/bitwise/bitwise020_1/#1728

- Think about how we could make "signed"/"unsigned" not be part of the type system, and instead treat integers as in assembly, where only the relevant _operations_ are separate for signed or unsigned. This poses problems in principle as its difficult for a programmer to trace
  signedness for unknown values (i.e: a parameter passed to a function), so think how the language/compiler could help with this and make it "transparent".
    · The simplest idea I can think of so far is have any quantities the compiler handles be signed numbers (i32, i64, etc.). That would be all literals, sizes, etc.
      Then a totally separate type with no direct conversion called 'bits' (b32, b64..) would be used for those cases that need flags, bitmasks, hashes, etc. There is no conversion
      between these whatsoever, other than a straight cast.
      Interacting with C would convert anything that fits to the next bigger iXX quantity up to i64, and where that's not possible (size_t), a runtime debug check similar to
      array bounds checking would be used.
- Compare against the overflow problems analyzed in https://graphitemaster.github.io/aau/?s=03

- First class discriminated unions, as a simple subtyping mechanism. Can be switched over in a case statement. Could be defined 'partial' to allow extending with new subtypes?.

- 'using' in structs & function args, to encourage composition (and for subtyping too).
  Allow passing a struct that is 'using' another to a function that expects the latter.
  TODO What about using through pointers?

- Reflection should be very natural, like in C#. There will be a special "member reference" token (#) to specifically access metadata for symbols and types.
  See how Ion does it at https://bitwise.handmade.network/episode/bitwise/bitwise019_1/#2274. Consider whether always having all type info is an actual problem (due to sheer size) and if it isn't make it even more integrated.

- We do want a Context so that we don't have to drag commonly used stuff across a ton of functions. However we don't want something dynamic and implicit and "hidden" like in JAI, but something more explicit and statically checked, and which can be examined at any time.
  I propose a very minimal set of fixed things in the default Context (for stuff that you really want to have _all_ the time, like an allocator), plus a separate arguments section on every function, with a type and a name for each arg, but where the name is _both_ the name
  that will be used inside the function body and also the name that will be "captured" from the calling scope, similar to lambdas in C++. Like lambdas, you just specify this in the function declaration, not everytime you call it.
  Unlike lambdas, the names apply to the _calling_ scope, not the declaring scope. In case the same name is not available in the calling scope, an error will be shown and an alias rule will have to be specified during the call.
  The caller can override the implicity defined Context arguments explicitly if he desires to do so too.

- What Jon Blow does where he's developing an actual "big" program (game) to soundly inform many decisions about the language is very very good, however I think it'd be more convenient for me to have several different "small to medium" programs instead of one.
  These should be programs solving disparate problems relevant in the respective domain(s) the language wants to address, and could even merge at some point if that makes sense. This would allow several things:
  - Have a broader picture of what the final application of the language among different technical areas will look like.
  - Provide an easy way to determine what needs to be moved over to the core libraries, as several of those programs will begin to show what pieces of functionality are needed over an over across areas.
  - Provide many pieces of real working code that can be easily converted into automated tests that can be added to a testsuite to continuously run during the whole language development and beyond.

- A way to declare an enum as a "flags" bitfield, with correct automatic values and convenient operations to test/set
  https://preshing.com/20150324/safe-bitfields-in-cpp/

- A way to tag pure functions? (see captures below)

- Member functions are ooooobviously forbidden! Prefer the "using model" of Jai, where function arguments can be preceded by 'using',
  which means their internals are available inside the function scope directly, without qualifying them. Also, we could extend that so
  that if a function has a first argument with using, we could invoke it with the 'object.func()' syntax, ALTHOUGH that may be
  dangerous already? (promotes the mental model of "this function belongs to this object")
- What about initializers ('constructors') though?

- Think about the "sticky error" pattern for error handling explained in https://youtu.be/0WpCnd9E-eg?t=5864 and how to apply it to
  the standard library, or even through language constructs.

- Initializers (both to provide an initial value and to indicate no initialization) can be specified in a struct declaration (default for all instances), or when declaring each instance.
  Anything not explicitly initialized, is automatically zero-initialized (unless marked to not do so, for example, by initializing to 'void').
  Something to think about is what happens with padding when a struct is specified as not even zero-initialized? The current C++ approach of leaving that out of the spec prevents defining a simple equality condition for structs as simple bit comparison, which I want to have
  (likewise for assignment).  Once you force padding to be zero-initialized regardless though.. what advantage there is to being able to not initialize the members?

- Do we want to disallow indexing pointers with [] (unchecked) so it feels less safe than indexing arrays (always checked)? (still allow pointer arithmetic ofc)

- 'Optional' types, specially for pointers? (https://youtu.be/TH9VCN6UkyQ?t=4689)

- Captures and "hard" captures, not just for functions but for simple code blocks too (https://youtu.be/5Nc68IdNKdg?t=3128) (more like "restricted scope" in this sense)
  This is perfect for pure functions!
  Does this "imply" closures?

- For interop with C, have two separate forms of #foreign directive: one for specifying functions and data types
  (which can be incomplete for opaque structs etc.) that will be usable from the program, which must reference a
  library by name/tag, and then a second form which describes those referenced names/tags, by listing the header
  files to include together with actual library filenames for each of the supported platforms, or even C++ source files
  to compile inline with the native code.

- A compile-time feature that would be extremely useful is constexpr monotonic counters (and being able to define _several_ of them).
  Think of how to do this in a simple way.

- Inlined enum decls to use in function arguments (where would that symbol live though?)

- Can we do a reliable almonst-equal operator for floats? (~=)

- Have a concept of scopes (file, module, global) for global declarations like in Jai, AND allow specifying them for aggregate members too. This allows for something like private attributes without the OOP stupidity.
- Other markers for attributes could be #private & #readonly. These would be paired with '#access Type', or '#access Type.attr', for explicitly marking code that can override that access (a la 'friend')

- I think the general idea of having constructor/destructor like we do in Glowmade allows for a lot of agility for everyday code compared to having to constantly be thinking about all the lifetimes and scopes etc.
  Think how this could be integrated in the language without bringing any of the RAII garbage along with it.

- Learn a bit about UTF8 and find out how many actual bytes a character literal can get to be.. seems its anything from b8 to b32

- Think of a sane way of implementing functors

- Investigate how to support hot code reloading at the binary level

- Add a directive for enums that forces you to provide values for each item ('explicit'?)

- Support data overloads in a struct for attributes of type function? This makes a lot of sense for function pointer tables, and in a sense it's just an extension of normal function overloading.
  (example?)

- Support some form of vectorization in the syntax (Casey mentioned the RiscV spec as a good example of how it should go)
  Have a look at what ISPC is doing

- Think about adding a compound expr that resolves to bits types, where each individual item is an enum flag or the negation of one.
    i.e: { YesFlag, ~NoFlag } would automatically resolve to the bits value equivalent to (YesFlag & ~NoFlag) with the appropriate promoted byte size

- Interesting allocation ideas from https://www.youtube.com/watch?v=LIb3L4vKZ7U
  · When allocating AND deallocating, always pass a buffer for the whole block instead of just a pointer to the beginning, to help reduce the need to keep track of all the block sizes, and simplify freeing (and probably enable patterns that would be hard to do otherwise)
  · A freelist could be a good companion to an arena allocator, to make it a bit more general. Any buffers freed could be added to several freelists split by size buckets. You could even think of making them an AVL tree instead of a list, to make it trivial to keep them sorted

- For directives that can be specified as scopes (affecting several lines of code), support two styles: #dir { ... } and #begin dir ... #end dir (which are checked against their pair at compile time ofc).
  The first one is super quick and convenient, the second one is self-documenting.

________________________________________________________________________________ 

More ideas:
- http://www.c3-lang.org/primer/


________________________________________________________________________________ 

Beyond the actual language features, something that I think most languages are quite poor at is debugging support and general dev tools. Several compiler features should strictly exist to aid during the day to day programming tasks, and in general all features, from basic
error messages to more advanced tools, should be designed with care so as to never be in the way of the programmer.

Second thing would be to make SIMD programming a first-class citizen in the language from the get-go. Writing SIMD code should be basically as intuitive as writing scalar code. For this, look for inspiration for a good portable model in stuff like HLSL, Agner Fog's vec library
and the Google Highway library. Main problem here ofc is how to provide good coverage and efficiency for all the available instruction sets under a common syntax.
Good interoperability with each platform's assembly is of course essential (meaning easy to use #asm blocks). For this learn a bit about the functionality and weird tricks & extensions to bare C provided by the AVR-GCC syntax
( https://www.felixcloutier.com/documents/gcc-asm.html & https://gcc.gnu.org/onlinedocs/gcc/Using-Assembly-Language-with-C.html#Using-Assembly-Language-with-C ).
**Even better**, have a #bytecode directive that allows inputting IRL blocks of vectorized code directly, which will be properly translated to native by the backend.

Of course, same thing to be said about multi-threading. No language should be designed anymore with single-threaded execution in mind, so all basic features from creating a thread or fiber to lock-free programming should be a basic part of the language (with more elaborated
facilities being provided in the standard library).


.. TOOLING ..

- Debuggers are very very poor at visualizing complex data in general, so there's tons to be improved here, from directly previewing images or other media, to graph-like representations of relationships between different parts of the data in memory.
  Some ideas on this regard: http://worrydream.com/#!/LearnableProgramming (see the adaptive visualization right before "Eliminate hidden state", for example)
  Also the Whitebox tool being developed by azmr (Handmade Net).

- One of the reasons profilers are incredibly useful is they allow to easily graph historical data, and debuggers have never tried something like that. There's probably a way to augment the debugged code directly so it can sample all kinds of stuff about the runtime behaviour
  of the code that can then be plotted directly in the debugger (maybe even provide a socket so that interesting data can be piped back to the app or to be viewed remotely).

- Something that could be a great aid while debugging the compiler itself, and which could potentially turn into its own thing,
  would be having a graphic visualization of the state of the running compiler with all the phases and internal structures visualized
  in ImGui. This could be a separate graphic app the compiler would connect to and feed it all its internal state at every moment, which
  would mean when breaking into the debugger during a crash or parse/resolve error or whatever, we'd have the full graphical display of
  all the internal info available besides the debugger's watch window.


________________________________________________________________________________

Interesting reads:
 - The death of optimizing compilers :: https://cr.yp.to/talks/2015.04.16/slides-djb-20150416-a4.pdf
 - Branch Prediction and the Performance of Interpreters -Don't Trust Folklore :: https://hal.inria.fr/hal-01100647/document


.. NAMES ..
I like the idea of having a composed name to represent the two major levels of abstraction / layers. For example:

- make/do :: .mk for when you really need to build stuff from the ground up / .do for when you're more focused on achieving and getting things done
  (although this makes it sound like there's two distinct syntaxes / compilers.. how would that even work?)

